las-pyramidal-multilingual-sequence8: !LoadSerialized
  filename: /home/jialu/xnmt/recipes/las-tedlium/models/las-pyramidal-multilingual-sequence7.mod
  overwrite:
   - path: evaluate
     val: null
   - path: train.tasks.0.src_file
     val: '{ENG_DATA_DIR}/feat/train8.h5'
   - path: train.tasks.0.trg_file
     val: '{ENG_DATA_DIR}/transcript/train8.char'
   - path: train.tasks.1.src_file
     val: '{CHI_DATA_DIR}/train1.h5'
   - path: train.tasks.1.trg_file
     val: '{CHI_DATA_DIR}/train1.char'
   # - path: train.tasks.0.batcher.avg_batch_size
   #   val: 65
   # - path: train.tasks.1.batcher.avg_batch_size
   #   val: 65
   # - path: train
   #   val: !SameBatchMultiTaskTrainingRegimen
   #    trainer: !AdamTrainer {alpha: 0.001}
   #    dev_zero: false
   #    tasks:
   #    - !SimpleTrainingTask
   #      name: english_task
   #      max_src_len: 1500
   #      max_trg_len: 350
   #      run_for_epochs: 1
   #      lr_decay: 0.5
   #      lr_decay_times: 3
   #      patience: 8
   #      initial_patience: 15
   #      dev_every: 1
   #      restart_trainer: true
   #      batcher: !WordSrcBatcher
   #        avg_batch_size: 50
   #        pad_src_to_multiple: 8
   #        src_pad_token: ~
   #      src_file: '{ENG_DATA_DIR}/feat/train1.h5'
   #      trg_file: '{ENG_DATA_DIR}/transcript/train1.char'
   #      dev_tasks:
   #        - !AccuracyEvalTask
   #          model: !Ref {name: english_model}
   #          eval_metrics: wer,cer
   #          src_file: &eng_dev_src '{ENG_DATA_DIR}/feat/dev.h5'
   #          ref_file: '{ENG_DATA_DIR}/transcript/dev.words'
   #          hyp_file: '{EXP_DIR}/logs/{EXP}.eng_dev_hyp'
   #          inference: !AutoRegressiveInference
   #            batcher: !InOrderBatcher
   #              batch_size: 1
   #              pad_src_to_multiple: 8
   #              src_pad_token: ~
   #            max_src_len: 1500
   #            post_process: join-char
   #            search_strategy: !BeamSearch
   #              max_len: 500
   #              beam_size: 20
   #              len_norm: !PolynomialNormalization
   #                apply_during_search: true
   #                m: 1.5
   #        - !LossEvalTask
   #          model: !Ref {name: english_model}
   #          max_src_len: 1500
   #          src_file: *eng_dev_src
   #          ref_file: '{ENG_DATA_DIR}/transcript/dev.char'
   #      model: !DefaultTranslator
   #        _xnmt_id: english_model
   #        src_embedder: !NoopEmbedder
   #          emb_dim: 40
   #        encoder: !ModularSeqTransducer
   #          _xnmt_id: shared_encoder
   #          modules:
   #            - !PyramidalLSTMSeqTransducer
   #              layers: 4
   #              reduce_factor: 2
   #              downsampling_method: concat
   #              input_dim: 40
   #              hidden_dim: 512
   #        attender: !MlpAttender
   #          hidden_dim: 128
   #        trg_embedder: !SimpleWordEmbedder
   #          emb_dim: 64
   #          word_dropout: 0.1
   #          fix_norm: 1
   #          vocab: !Ref {name: trg_eng_vocab}
   #        decoder: !AutoRegressiveDecoder
   #          rnn: !UniLSTMSeqTransducer
   #            layers: 1
   #            hidden_dim: 512
   #          input_feeding: True
   #          bridge: !CopyBridge {}
   #          scorer: !Softmax
   #            vocab: !Ref {name: trg_eng_vocab}
   #            label_smoothing: 0.1
   #        src_reader: !H5Reader
   #          transpose: true
   #        trg_reader: !PlainTextReader
   #          vocab: !Vocab
   #            _xnmt_id: trg_eng_vocab
   #            vocab_file: '{EXP_DIR}/vocab.char'
   #    - !SimpleTrainingTask
   #      name: mandarin_task
   #      max_src_len: 1500
   #      max_trg_len: 350
   #      run_for_epochs: 1
   #      lr_decay: 0.5
   #      lr_decay_times: 3
   #      patience: 8
   #      initial_patience: 15
   #      dev_every: 1
   #      restart_trainer: true
   #      batcher: !WordSrcBatcher
   #        avg_batch_size: 50
   #        pad_src_to_multiple: 8
   #        src_pad_token: ~
   #      src_file: '{CHI_DATA_DIR}/train1.h5'
   #      trg_file: '{CHI_DATA_DIR}/train1.char'
   #      dev_tasks:
   #        - !AccuracyEvalTask
   #          model: !Ref {name: mandarin_model}
   #          eval_metrics: wer,cer
   #          src_file: &chi_dev_src '{CHI_DATA_DIR}/dev.h5'
   #          ref_file: '{CHI_DATA_DIR}/dev.words'
   #          hyp_file: '{EXP_DIR}/logs/{EXP}.chi_dev_hyp'
   #          inference: !AutoRegressiveInference
   #            batcher: !InOrderBatcher
   #              batch_size: 1
   #              pad_src_to_multiple: 8
   #              src_pad_token: ~
   #            max_src_len: 1500
   #            post_process: join-char
   #            search_strategy: !BeamSearch
   #              max_len: 500
   #              beam_size: 20
   #              len_norm: !PolynomialNormalization
   #                apply_during_search: true
   #                m: 1.5
   #        - !LossEvalTask
   #          model: !Ref {name: mandarin_model}
   #          max_src_len: 1500
   #          src_file: *chi_dev_src
   #          ref_file: '{CHI_DATA_DIR}/dev.char'
   #      model: !DefaultTranslator
   #        _xnmt_id: mandarin_model
   #        src_embedder: !NoopEmbedder
   #          emb_dim: 40
   #        encoder: !Ref {name: shared_encoder}
   #        attender: !MlpAttender
   #          hidden_dim: 128
   #        trg_embedder: !SimpleWordEmbedder
   #          emb_dim: 64
   #          word_dropout: 0.1
   #          fix_norm: 1
   #          vocab: !Ref {name: trg_chi_vocab}
   #        decoder: !AutoRegressiveDecoder
   #          rnn: !UniLSTMSeqTransducer
   #            layers: 1
   #            hidden_dim: 512
   #          input_feeding: true
   #          bridge: !CopyBridge {}
   #          scorer: !Softmax
   #            vocab: !Ref {name: trg_chi_vocab}
   #            label_smoothing: 0.1
   #        src_reader: !H5Reader
   #          transpose: true
   #        trg_reader: !PlainTextReader
   #          vocab: !Vocab
   #            _xnmt_id: trg_chi_vocab
   #            vocab_file: '{EXP_DIR}/chineseVocab.char'
   # - path: evaluate
   #   val: null
   #  # val:
   #  #   - !AccuracyEvalTask
   #  #     model: !Ref { name: english_model }
   #  #     eval_metrics: wer,cer
   #  #     src_file: '{ENG_DATA_DIR}/feat/test.h5'
   #  #     ref_file: '{ENG_DATA_DIR}/transcript/test.words'
   #  #     hyp_file: '{EXP_DIR}/logs/{EXP}.eng_test_hyp'
   #  #     inference: !AutoRegressiveInference
   #  #       batcher: !InOrderBatcher
   #  #         batch_size: 1
   #  #         pad_src_to_multiple: 8
   #  #         src_pad_token: ~
   #  #       max_src_len: 1500
   #  #       post_process: join-char
   #  #       search_strategy: !BeamSearch
   #  #         max_len: 500
   #  #         beam_size: 20
   #  #         len_norm: !PolynomialNormalization
   #  #           apply_during_search: true
   #  #           m: 1.5
   #  #   - !AccuracyEvalTask
   #  #     model: !Ref { name: mandarin_model }
   #  #     eval_metrics: wer,cer
   #  #     src_file: '{CHI_DATA_DIR}/test.h5'
   #  #     ref_file: '{CHI_DATA_DIR}/test.words'
   #  #     hyp_file: '{EXP_DIR}/logs/{EXP}.chi_test_hyp'
   #  #     inference: !AutoRegressiveInference
   #  #       batcher: !InOrderBatcher
   #  #         batch_size: 1
   #  #         pad_src_to_multiple: 8
   #  #         src_pad_token: ~
   #  #       max_src_len: 1500
   #  #       post_process: join-char
   #  #       search_strategy: !BeamSearch
   #  #         max_len: 500
   #  #         beam_size: 20
   #  #         len_norm: !PolynomialNormalization
   #  #           apply_during_search: true
   #  #           m: 1.5
   #  #   - !LossEvalTask
   #  #     model: !Ref {name: mandarin_model}
   #  #     max_src_len: 1500
   #  #     src_file: '{CHI_DATA_DIR}/test.h5'
   #  #     ref_file: '{CHI_DATA_DIR}/test.char'
   #  #   - !LossEvalTask
   #  #     model: !Ref {name: english_model}
   #  #     max_src_len: 1500
   #  #     src_file: '{ENG_DATA_DIR}/feat/test.h5'
   #  #     ref_file: '{ENG_DATA_DIR}/transcript/test.char'
